{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gluon-cv_PoseEstimation.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1AMgpfvgXknX0Ro1ySpE0ygCpJSrqQ0WJ","authorship_tag":"ABX9TyPcrKd4pTYk1w1lxLZun58U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kxJG_36AH_CW","colab_type":"code","colab":{}},"source":["!pip install mxnet-cu100 gluoncv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8X9dg26QJ5ok","colab_type":"code","colab":{}},"source":["import cv2\n","import pickle\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","import mxnet as mx\n","from datetime import datetime\n","from matplotlib import pyplot as plt\n","from gluoncv import model_zoo, data, utils\n","from gluoncv.data.transforms.pose import detector_to_simple_pose, heatmap_to_coord"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x49bGM9cKNNw","colab_type":"text"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"9qBUv9E8KJ9V","colab_type":"code","colab":{}},"source":["def resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n","    # initialize the dimensions of the image to be resized and\n","    # grab the image size\n","    dim = None\n","    (h, w) = image.shape[:2]\n","\n","    # if both the width and height are None, then return the\n","    # original image\n","    if width is None and height is None:\n","        return image\n","\n","    if width and height:\n","        dim = (width, height)\n","    elif width is None:\n","        # calculate the ratio of the height and construct the dimensions\n","        r = height / float(h)\n","        dim = (int(w * r), height)\n","    elif height is None:\n","        # calculate the ratio of the width and construct the dimensions\n","        r = width / float(w)\n","        dim = (width, int(h * r))\n","\n","    # resize the image\n","    resized = cv2.resize(image, dim, interpolation=inter)\n","\n","    # return the resized image\n","    return resized"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfOHG_2UITc-","colab_type":"text"},"source":["### Example\n","\n","Using sample image with pre-trained Yolo3 model and pre-trained SimplePoseNet."]},{"cell_type":"code","metadata":{"id":"_r-jGDKKITQp","colab_type":"code","colab":{}},"source":["detector = model_zoo.get_model('yolo3_mobilenet1.0_coco', pretrained=True)\n","pose_net = model_zoo.get_model('simple_pose_resnet18_v1b', pretrained=True)\n","\n","# Note that we can reset the classes of the detector to only include\n","# human, so that the NMS process is faster.\n","\n","detector.reset_class([\"person\"], reuse_weights=['person'])\n","\n","im_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n","                          'gluoncv/pose/soccer.png?raw=true',\n","                          path='soccer.png')\n","x, img = data.transforms.presets.ssd.load_test(im_fname, short=512)\n","print('Shape of pre-processed image:', x.shape)\n","\n","class_IDs, scores, bounding_boxs = detector(x)\n","\n","pose_input, upscale_bbox = detector_to_simple_pose(img, class_IDs, scores, bounding_boxs)\n","\n","predicted_heatmap = pose_net(pose_input)\n","pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)\n","\n","ax = utils.viz.plot_keypoints(img, pred_coords, confidence,\n","                              class_IDs, bounding_boxs, scores,\n","                              box_thresh=0.5, keypoint_thresh=0.2)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7_V6OVyhIYSr","colab_type":"text"},"source":["### Custom Example\n","\n","Using sample image with fine-tunned Detectron2 model and pre-trained SimplePoseNet."]},{"cell_type":"code","metadata":{"id":"cjATp-5ZVFKf","colab_type":"code","colab":{}},"source":["pose_net = model_zoo.get_model('simple_pose_resnet18_v1b', pretrained=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OV9r7d9zKdRm","colab_type":"code","colab":{}},"source":["img_filename = '/content/drive/My Drive/data/detectron/tolpum_0-15_130.jpg'\n","img = cv2.imread(img_filename)\n","img_resized = resize(img, width=1024)\n","print(img_resized.shape)\n","\n","## These are the detections at the original image size\n","detections = [\n","    [634.0, 770.0, 109.0, 196.0],\n","    [976.0, 462.0, 61.0, 131.0],\n","    [113.0, 230.0, 71.0, 103.0],\n","    [216.0, 488.0, 81.0, 152.0]\n","]\n","\n","for detection in detections:\n","    x1, y1, width, height = detection\n","    x2 = x1 + width\n","    y2 = y1 + height\n","\n","    ## Since we resized the image, we also need to adapt the detections\n","    x1 = (img_resized.shape[0] * x1) / img.shape[0]\n","    x2 = (img_resized.shape[0] * x2) / img.shape[0]\n","    y1 = (img_resized.shape[1] * y1) / img.shape[1]\n","    y2 = (img_resized.shape[1] * y2) / img.shape[1]\n","\n","    print(x1, y1, x2, y2)\n","\n","    cv2.rectangle(img_resized, (int(x1), int(y1)), (int(x2), int(y2)), (60, 71, 222), 5)\n","    break\n","\n","cv2_imshow(img_resized)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"64lDaE5TIiq4","colab_type":"code","colab":{}},"source":["## For some reason, it expects as input the following:\n","## custom_class_IDs with shape (1, 100, 1)\n","## custom_scores with shape (1, 100, 1)\n","## custom_bounding_boxs with shape (1, 100, 4)\n","\n","# Represents the ClassID for each bbox (in this case just one class with an ID equal to 0), while the rest is filled up with -1\n","custom_class_IDs = [0]\n","N = 100 - len(custom_class_IDs)\n","custom_class_IDs = np.pad(custom_class_IDs, (0, N), 'constant', constant_values=(-1))\n","custom_class_IDs = np.expand_dims(custom_class_IDs, axis=1)\n","custom_class_IDs = np.expand_dims(custom_class_IDs, axis=0)\n","custom_class_IDs = mx.nd.array(custom_class_IDs)\n","\n","# Represents Score for each bbox (in this case juse one score), while the rest is filled up with -1\n","custom_scores = [0.95827]\n","N = 100 - len(custom_scores)\n","custom_scores = np.pad(custom_scores, (0, N), 'constant', constant_values=(-1))\n","custom_scores = np.expand_dims(custom_scores, axis=1)\n","custom_scores = np.expand_dims(custom_scores, axis=0)\n","custom_scores = mx.nd.array(custom_scores)\n","\n","# Represents the bbox for each object (in this case juse one bbox), while the rest is filled up with -1\n","custom_bounding_boxs = [[338.1333333333333, 410.6666666666667, 396.26666666666665, 515.2]]\n","N = 100 - len(custom_bounding_boxs)\n","custom_bounding_boxs = np.pad(custom_bounding_boxs, ((0, N), (0, 0)), 'constant', constant_values=(-1))\n","custom_bounding_boxs = np.expand_dims(custom_bounding_boxs, axis=0)\n","custom_bounding_boxs = mx.nd.array(custom_bounding_boxs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"puA97nEmJg7i","colab_type":"code","colab":{}},"source":["## Optional but useful to compare shapes/types between first example and second example\n","assert class_IDs.shape == custom_class_IDs.shape and type(class_IDs) == type(custom_class_IDs)\n","assert scores.shape == custom_scores.shape and type(scores) == type(custom_scores)\n","assert bounding_boxs.shape == custom_bounding_boxs.shape and type(bounding_boxs) == type(custom_bounding_boxs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SUmmXq7Jyui","colab_type":"code","colab":{}},"source":["img_filename = '/content/drive/My Drive/data/detectron/tolpum_0-15_130.jpg'\n","\n","img = cv2.imread(img_filename)\n","img_resized = resize(img, width=1024)\n","print(img_resized.shape)\n","\n","pose_input, upscale_bbox = detector_to_simple_pose(img_resized, custom_class_IDs, custom_scores, custom_bounding_boxs)\n","predicted_heatmap = pose_net(pose_input)\n","pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)\n","\n","res = utils.viz.cv_plot_keypoints(img_resized, pred_coords, confidence,\n","                              custom_class_IDs, custom_bounding_boxs, custom_scores,\n","                              box_thresh=0.5, keypoint_thresh=0.2)\n","\n","cv2_imshow(res)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTpnATX-jQqy","colab_type":"text"},"source":["### Video"]},{"cell_type":"code","metadata":{"id":"PQs4BRHmtm-P","colab_type":"code","colab":{}},"source":["## Draw Pose\n","def draw_keypoints(img, coords, confidence, class_ids, bboxes, scores, box_thresh=0.5, keypoint_thresh=0.2, scale=1.0, **kwargs):\n","\n","    custom_colors = [(255, 255, 255), (235, 208, 52), (8, 0, 255)]\n","\n","    if isinstance(img, mx.nd.NDArray):\n","        img = img.asnumpy()\n","    if isinstance(coords, mx.nd.NDArray):\n","        coords = coords.asnumpy()\n","    if isinstance(class_ids, mx.nd.NDArray):\n","        class_ids = class_ids.asnumpy()\n","    if isinstance(bboxes, mx.nd.NDArray):\n","        bboxes = bboxes.asnumpy()\n","    if isinstance(scores, mx.nd.NDArray):\n","        scores = scores.asnumpy()\n","    if isinstance(confidence, mx.nd.NDArray):\n","        confidence = confidence.asnumpy()\n","\n","    custom_colors = [(255, 255, 255), (235, 208, 52), (8, 0, 255)]\n","\n","    joint_visible = confidence[:, :, 0] > keypoint_thresh\n","    joint_pairs = [[0, 1], [1, 3], [0, 2], [2, 4],\n","                    [5, 6], [5, 7], [7, 9], [6, 8], [8, 10],\n","                    [5, 11], [6, 12], [11, 12],\n","                    [11, 13], [12, 14], [13, 15], [14, 16]]\n","\n","    colormap_index = np.linspace(0, 1, len(joint_pairs))\n","    coords *= scale\n","    for i in range(coords.shape[0]):\n","        pts = coords[i]\n","        custom_color = custom_colors[class_ids[i]]\n","        for cm_ind, jp in zip(colormap_index, joint_pairs):\n","            if joint_visible[i, jp[0]] and joint_visible[i, jp[1]]:\n","                if class_ids[i] == 0:\n","                    cm_color = tuple([int(x * 255) for x in plt.cm.hot(cm_ind)[:3]])\n","                else:\n","                    cm_color = tuple([int(x * 255) for x in plt.cm.cool(cm_ind)[:3]])\n","                pt1 = (int(pts[jp, 0][0]), int(pts[jp, 1][0]))\n","                pt2 = (int(pts[jp, 0][1]), int(pts[jp, 1][1]))\n","                cv2.line(img, pt1, pt2, cm_color, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41vPhSXnklta","colab_type":"text"},"source":["Using Detections from Detectron2"]},{"cell_type":"code","metadata":{"id":"aobeWZSJjQRe","colab_type":"code","colab":{}},"source":["input_video_filename = '/content/drive/My Drive/data/detectron/TolucaVSPumas_sample.mp4'\n","input_video = cv2.VideoCapture(input_video_filename)\n","\n","with open('/content/drive/My Drive/data/detectron/team_detections.pickle', 'rb') as fp:\n","    frames_detections = pickle.load(fp)\n","\n","num_frames = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n","width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","frames_per_second = input_video.get(cv2.CAP_PROP_FPS)\n","num_frames = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","output_video_filename = '/content/pose_estimation.mp4'\n","output_video = cv2.VideoWriter(filename=output_video_filename, \n","                               fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n","                               fps=float(frames_per_second),\n","                               frameSize=(1024, 576), ## (width, height) -- MAKE SURE TO MODIFY DEPENDING ON RESIZE\n","                               isColor=True)\n","\n","i = 0\n","while input_video.isOpened():\n","\n","    if i % 60 == 0:\n","        prog = round(i / num_frames, 2) * 100\n","        print('{} | {}%'.format(datetime.now(), prog))\n","\n","    ret, frame = input_video.read()\n","    if ret:\n","\n","        frame_resized = resize(frame, width=1024)\n","        \n","        for frame_detections in frames_detections:\n","            if frame_detections[0] == i:\n","\n","                number_detections = len(frame_detections[1])\n","                N = 100 - number_detections\n","\n","                # Since we know beforehand they all belong to the same class, we can do the following:\n","                # Note that the PoseEstimation only accepts a ClassID = 0\n","                class_IDs = np.zeros(number_detections) # frame_detections[3]\n","                class_IDs = np.pad(class_IDs, (0, N), 'constant', constant_values=(-1))\n","                class_IDs = np.expand_dims(class_IDs, axis=1)\n","                class_IDs = np.expand_dims(class_IDs, axis=0)\n","                class_IDs = mx.nd.array(class_IDs)\n","                \n","                # Since we've already filtered the detections by score, we can do the following:\n","                scores = frame_detections[2]\n","                scores = np.pad(scores, (0, N), 'constant', constant_values=(-1))\n","                scores = np.expand_dims(scores, axis=1)\n","                scores = np.expand_dims(scores, axis=0)\n","                scores = mx.nd.array(scores)\n","\n","                bounding_boxs = []\n","                for idx, detections in enumerate(frame_detections[1]):\n","\n","                    x1, y1, x2, y2 = detections\n","                    width = x2 - x1\n","                    height = y2 - y1\n","                    x1 = (frame_resized.shape[0] * x1) / frame.shape[0]\n","                    x2 = (frame_resized.shape[0] * x2) / frame.shape[0]\n","                    y1 = (frame_resized.shape[1] * y1) / frame.shape[1]\n","                    y2 = (frame_resized.shape[1] * y2) / frame.shape[1]\n","                    bounding_boxs.append([x1, y1, x2, y2])\n","                    # cv2.rectangle(frame_resized, (int(x1), int(y1)), (int(x2), int(y2)), (60, 71, 222), 5)\n","\n","                if bounding_boxs:\n","                    # Prepare bbox input size for model\n","                    # N = 100 - len(bounding_boxs)\n","                    bounding_boxs = np.pad(bounding_boxs, ((0, N), (0, 0)), 'constant', constant_values=(-1))\n","                    bounding_boxs = np.expand_dims(bounding_boxs, axis=0)\n","                    bounding_boxs = mx.nd.array(bounding_boxs)\n","\n","                    # Run model\n","                    pose_input, upscale_bbox = detector_to_simple_pose(frame_resized, class_IDs, scores, bounding_boxs)\n","                    predicted_heatmap = pose_net(pose_input)\n","                    pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)\n","\n","                    # Draw pose\n","                    # res = utils.viz.cv_plot_keypoints(frame_resized, pred_coords, confidence, class_IDs, bounding_boxs, scores)\n","                    draw_keypoints(frame_resized, pred_coords, confidence, frame_detections[3], bounding_boxs, scores)\n","\n","                break\n","\n","        output_video.write(frame_resized)\n","        i += 1\n","    else:\n","        break\n","\n","input_video.release()\n","output_video.release()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zl8LaqtgkMqS","colab_type":"text"},"source":["Using Tracks from IOU Tracker"]},{"cell_type":"code","metadata":{"id":"DfEPw-HEcYj9","colab_type":"code","colab":{}},"source":["input_video_filename = '/content/drive/My Drive/data/detectron/TolucaVSPumas_sample.mp4'\n","input_video = cv2.VideoCapture(input_video_filename)\n","\n","with open('/content/drive/My Drive/data/detectron/tracks.pickle', 'rb') as fp:\n","    tracks = pickle.load(fp)\n","\n","num_frames = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n","width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","frames_per_second = input_video.get(cv2.CAP_PROP_FPS)\n","num_frames = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","output_video_filename = '/content/pose_estimation.mp4'\n","output_video = cv2.VideoWriter(filename=output_video_filename, \n","                               fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n","                               fps=float(frames_per_second),\n","                               frameSize=(1024, 576), ## (width, height) -- MAKE SURE TO MODIFY DEPENDING ON RESIZE\n","                               isColor=True)\n","\n","i = 0\n","while input_video.isOpened():\n","\n","    if i % 60 == 0:\n","        prog = round(i / num_frames, 2) * 100\n","        print('{} | {}%'.format(datetime.now(), prog))\n","\n","    ret, frame = input_video.read()\n","    if ret:\n","        \n","        frame_resized = resize(frame, width=1024)\n","\n","        bounding_boxs = []\n","        classes = []\n","        for t in tracks:\n","            if t['id'] in [30, 31, 32, 33, 47, 50]:\n","                continue\n","            for bbox in t['bbox']:\n","                ## bbox[0] represents the frame number\n","                if bbox[0] == i:\n","                    x1, y1, x2, y2 = bbox[-1].tolist()\n","                    width = x2 - x1\n","                    height = y2 - y1\n","                    x1 = (frame_resized.shape[0] * x1) / frame.shape[0]\n","                    x2 = (frame_resized.shape[0] * x2) / frame.shape[0]\n","                    y1 = (frame_resized.shape[1] * y1) / frame.shape[1]\n","                    y2 = (frame_resized.shape[1] * y2) / frame.shape[1]\n","                    bounding_boxs.append([x1, y1, x2, y2])\n","                    classes.append(bbox[2])\n","\n","        if bounding_boxs:\n","            number_detections = len(bounding_boxs)\n","            N = 100 - number_detections\n","\n","            # Since we know beforehand they all belong to the same class, we can do the following:\n","            # Note that the PoseEstimation only accepts a ClassID = 0\n","            class_IDs = np.zeros(number_detections)\n","            class_IDs = np.pad(class_IDs, (0, N), 'constant', constant_values=(-1))\n","            class_IDs = np.expand_dims(class_IDs, axis=1)\n","            class_IDs = np.expand_dims(class_IDs, axis=0)\n","            class_IDs = mx.nd.array(class_IDs)\n","            \n","            # Since we've already filtered the detections by score, we can do the following:\n","            scores = np.ones(number_detections)\n","            scores = np.pad(scores, (0, N), 'constant', constant_values=(-1))\n","            scores = np.expand_dims(scores, axis=1)\n","            scores = np.expand_dims(scores, axis=0)\n","            scores = mx.nd.array(scores)\n","\n","            # Prepare bbox input size for model\n","            bounding_boxs = np.pad(bounding_boxs, ((0, N), (0, 0)), 'constant', constant_values=(-1))\n","            bounding_boxs = np.expand_dims(bounding_boxs, axis=0)\n","            bounding_boxs = mx.nd.array(bounding_boxs)\n","\n","            # Run model\n","            pose_input, upscale_bbox = detector_to_simple_pose(frame_resized, class_IDs, scores, bounding_boxs)\n","            predicted_heatmap = pose_net(pose_input)\n","            pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)\n","\n","            # Draw pose\n","            # res = utils.viz.cv_plot_keypoints(frame_resized, pred_coords, confidence, class_IDs, bounding_boxs, scores)\n","            draw_keypoints(frame_resized, pred_coords, confidence, classes, bounding_boxs, scores)\n","\n","            # cv2.imwrite('test.jpg', frame_resized)\n","            output_video.write(frame_resized)\n","\n","        i += 1\n","    else:\n","        break\n","\n","input_video.release()\n","output_video.release()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_HVfNDNj6Pz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
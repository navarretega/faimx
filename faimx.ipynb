{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"faimx.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Btqvbon7VD9MNtIhNno791_DpWdhTOy3","authorship_tag":"ABX9TyN/PoshNhd3JnhbLoT3Ja5p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zu-QqgWreJlq","colab_type":"text"},"source":["## FAIMX\n","\n","Here we're using the following video as an example:<br>\n","**TolucaVSPumas_sample.mp4**"]},{"cell_type":"code","metadata":{"id":"q4yR0rgBYc5t","colab_type":"code","colab":{}},"source":["import pickle\n","import cv2\n","import os\n","import numpy as np\n","from scipy.signal import savgol_filter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3KQOv0obcj48","colab_type":"code","colab":{}},"source":["video = \"drive/My Drive/data/faimx/TolucaVSPumas_sample.mp4\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7CxzSeujYnXb","colab_type":"text"},"source":["### 1. Object Detection using Detectron2\n","\n","The idea is to detect teams and referees.<br>\n","See **Detectron2.ipynb** for details.<br>\n","The output is a pickle file (list) with all the detections for each frame on a video."]},{"cell_type":"code","metadata":{"id":"4QZ2m3voZC-K","colab_type":"code","colab":{}},"source":["team_detections_file = \"drive/My Drive/data/faimx/team_detections.pickle\"\n","with open(team_detections_file, 'rb') as fp:\n","    frames_detections = pickle.load(fp)\n","\n","print('Total Frames: {}'.format(len(frames_detections)))\n","for i, frame_detections in enumerate(frames_detections):\n","    print(frame_detections)\n","    if i == 8:\n","        break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jI7n8AIZDhU","colab_type":"text"},"source":["### 2. Run IOU Tracker\n","Here we want to run the tracker because we want to smooth each player trajectory, otherwise there's too much jitter and movement. <br>\n","See **scripts/iou_tracker.py** for more details. <br>\n","The input are the predictions from above while the output is anoter pickle file (list) with all the tracks associated for each detection. "]},{"cell_type":"code","metadata":{"id":"MRtWg-IIarBi","colab_type":"code","colab":{}},"source":["tracks_file = \"drive/My Drive/data/faimx/tracks.pickle\"\n","with open(tracks_file, 'rb') as fp:\n","    tracks = pickle.load(fp)\n","\n","print('Total Tracks: {}'.format(len(tracks)))\n","for i, track in enumerate(tracks):\n","    print(track)\n","    if i == 1:\n","        break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rD3Jgd12bDNO","colab_type":"text"},"source":["Optionally, you can use the functions **visualize_video()** and **plot_graph()** to visualize the intermediate results."]},{"cell_type":"markdown","metadata":{"id":"6eJd65xzbTB6","colab_type":"text"},"source":["### 3. Convert to a 2D map representation.  \n","Before running this, you need to make sure you have the necessary Homography matrices to transform the images. Otherwise it won't be possible. <br>\n","See **Pitch.ipynb** at the end for more details.\n","\n","The input are the Original Tracks while the output is another pickle file (list) with the Map Tracks.\n","\n","Remember that they are different because they have different X,Y coordinates. So, basically the Map Tracks represent the transformation of coordinates from the Original Tracks using Homography matrices. \n"]},{"cell_type":"code","metadata":{"id":"qjQ19iB3c2-9","colab_type":"code","colab":{}},"source":["map_tracks_file = \"drive/My Drive/data/faimx/map_tracks.pickle\"\n","with open(map_tracks_file, 'rb') as fp:\n","    map_tracks = pickle.load(fp)\n","\n","print('Total Tracks: {}'.format(len(map_tracks)))\n","for i, map_track in enumerate(map_tracks):\n","    print(map_track)\n","    if i == 1:\n","        break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-LLYirkdy1K","colab_type":"text"},"source":["Optionally, you can use the function **visualize_pitch()** and **plot_graph()** to visualize the intermediate results."]},{"cell_type":"markdown","metadata":{"id":"Yvw_VklrQdmP","colab_type":"text"},"source":["### 4. Run Pose Estimation\n","\n","See **Gluon-cv_PoseEstimation.ipynb** for details.\n","\n","The Detectron2 detections or the IOU Tracks are used as input (pickle file) and the Output is the video with the Pose Estimations drawn."]},{"cell_type":"markdown","metadata":{"id":"hTyX2SDDdVgZ","colab_type":"text"},"source":["### 4. Visualizing results\n","\n","Here we are just adding the pitch over the videos generated before. "]},{"cell_type":"code","metadata":{"id":"rxDm4iLUhL1s","colab_type":"code","colab":{}},"source":["## Utils\n","\n","def resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n","    # initialize the dimensions of the image to be resized and\n","    # grab the image size\n","    dim = None\n","    (h, w) = image.shape[:2]\n","\n","    # if both the width and height are None, then return the\n","    # original image\n","    if width is None and height is None:\n","        return image\n","\n","    if width and height:\n","        dim = (width, height)\n","    elif width is None:\n","        # calculate the ratio of the height and construct the dimensions\n","        r = height / float(h)\n","        dim = (int(w * r), height)\n","    elif height is None:\n","        # calculate the ratio of the width and construct the dimensions\n","        r = width / float(w)\n","        dim = (width, int(h * r))\n","\n","    # resize the image\n","    resized = cv2.resize(image, dim, interpolation=inter)\n","\n","    # return the resized image\n","    return resized"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Em4vIus5dYIl","colab_type":"code","outputId":"15e0a4ec-9741-4cee-dd85-38f7800ce312","executionInfo":{"status":"ok","timestamp":1591209280831,"user_tz":300,"elapsed":38766,"user":{"displayName":"Alejandro Navarrete GarcÃ­a","photoUrl":"","userId":"08773364630357935430"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["input_video_filename = 'drive/My Drive/data/faimx/pose_estimation.mp4'\n","input_video = cv2.VideoCapture(input_video_filename)\n","num_frames = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n","width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","frames_per_second = input_video.get(cv2.CAP_PROP_FPS)\n","num_frames = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","output_video_filename = 'drive/My Drive/data/faimx/faimx.mp4'\n","output_video = cv2.VideoWriter(filename=output_video_filename, fourcc=cv2.VideoWriter_fourcc(*'mp4v'), fps=float(frames_per_second), frameSize=(width, height), isColor=True)\n","\n","## Smooth data\n","data = []\n","for t in map_tracks:\n","    ids = []\n","    x_avgs = []\n","    y_avgs = []\n","    frame_numbers = []\n","    scores = []\n","    class_ids = []\n","    print(t['id'], len(t['bbox']))\n","    for bbox in t['bbox']:\n","\n","        ids.append(t['id'])\n","        x_avgs.append(bbox[-1][0][0])\n","        y_avgs.append(bbox[-1][1][0])\n","        frame_numbers.append(bbox[0])\n","        scores.append(bbox[1])\n","        class_ids.append(bbox[2])\n","\n","    data.append({\n","        'ids': ids,\n","        'x_avgs': x_avgs,\n","        'y_avgs': y_avgs,\n","        'frame_numbers': frame_numbers,\n","        'scores': scores,\n","        'class_ids': class_ids\n","    })\n","\n","    for d in data:\n","        # print(set(d['ids']), len(d['x_avgs']), len(d['y_avgs']))\n","        assert len(d['x_avgs']) == len(d['y_avgs'])\n","        if len(d['x_avgs']) > 400:\n","            d['x_avgs_smooth'] = savgol_filter(d['x_avgs'], 199, 3)\n","            d['y_avgs_smooth'] = savgol_filter(d['y_avgs'], 199, 3)\n","        elif len(d['x_avgs']) > 200:\n","            d['x_avgs_smooth'] = savgol_filter(d['x_avgs'], 99, 3)\n","            d['y_avgs_smooth'] = savgol_filter(d['y_avgs'], 99, 3)\n","        elif len(d['x_avgs']) > 100:\n","            d['x_avgs_smooth'] = savgol_filter(d['x_avgs'], 49, 3)\n","            d['y_avgs_smooth'] = savgol_filter(d['y_avgs'], 49, 3)\n","        elif len(d['x_avgs']) > 50:\n","            d['x_avgs_smooth'] = savgol_filter(d['x_avgs'], 23, 3)\n","            d['y_avgs_smooth'] = savgol_filter(d['y_avgs'], 23, 3)\n","        elif len(d['x_avgs']) > 25:\n","            d['x_avgs_smooth'] = savgol_filter(d['x_avgs'], 11, 3)\n","            d['y_avgs_smooth'] = savgol_filter(d['y_avgs'], 11, 3)\n","        elif len(d['x_avgs']) > 12:\n","            d['x_avgs_smooth'] = savgol_filter(d['x_avgs'], 5, 3)\n","            d['y_avgs_smooth'] = savgol_filter(d['y_avgs'], 5, 3)\n","        else:\n","            d['x_avgs_smooth'] = d['x_avgs']\n","            d['y_avgs_smooth'] = d['y_avgs']\n","        \n","## END Smooth data\n","\n","bbox_colors = [(255, 255, 255), (235, 208, 52), (8, 0, 255)]\n","i = 0\n","while input_video.isOpened():\n","\n","    if i % 60 == 0:\n","        prog = round(i / num_frames, 2) * 100\n","        print('{}%'.format(prog))\n","\n","    ret, frame = input_video.read()\n","\n","    if ret:\n","\n","        pitch = cv2.imread('drive/My Drive/data/faimx/pitch.png')\n","\n","        for d in data:\n","            for fid, frame_number, x_avg, y_avg, score, class_id in zip(d['ids'], d['frame_numbers'], d['x_avgs_smooth'], d['y_avgs_smooth'], d['scores'], d['class_ids']):\n","                if fid in [30, 31, 32, 33, 47, 50]:\n","                    continue\n","                if frame_number == i:\n","                    bbox_color = bbox_colors[class_id]\n","                    cv2.circle(pitch, (int(x_avg), int(y_avg)), 10, bbox_color, -1)\n","\n","        pitch = resize(pitch, height=210)\n","        x_position = frame.shape[0] - pitch.shape[0] - 10 ## Around bottom of image\n","        y_position = int((frame.shape[1] - pitch.shape[1]) / 2) ## Around center of image\n","        blend = 0.7\n","\n","        rows, cols, channels = pitch.shape\n","        pitch = cv2.addWeighted(frame[x_position:x_position+rows, y_position:y_position+cols], blend, pitch, blend, 0)\n","        frame[x_position:x_position+rows, y_position:y_position+cols] = pitch\n","\n","        # cv2.imwrite('/home/alex/Downloads/pitch/result/{}.jpg'.format(i), frame)\n","\n","        output_video.write(frame)\n","        i += 1\n","\n","    else:\n","        break\n","\n","input_video.release()\n","output_video.release()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["1 588\n","2 358\n","3 653\n","4 366\n","5 359\n","6 181\n","7 642\n","8 642\n","9 617\n","10 490\n","11 96\n","12 384\n","13 413\n","14 852\n","15 2\n","16 520\n","17 368\n","18 1\n","19 244\n","20 404\n","21 1\n","22 407\n","23 547\n","24 392\n","25 206\n","26 299\n","27 122\n","28 154\n","29 64\n","30 6\n","31 48\n","32 1\n","33 18\n","34 340\n","35 122\n","36 73\n","37 62\n","38 294\n","39 3\n","40 288\n","41 273\n","42 269\n","43 10\n","44 13\n","45 194\n","46 182\n","47 5\n","48 3\n","49 6\n","50 8\n","51 1\n","52 6\n","0.0%\n","5.0%\n","10.0%\n","15.0%\n","20.0%\n","25.0%\n","30.0%\n","35.0%\n","40.0%\n","45.0%\n","50.0%\n","55.00000000000001%\n","60.0%\n","65.0%\n","71.0%\n","76.0%\n","81.0%\n","86.0%\n","91.0%\n","96.0%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wCDH1rgvQx7-","colab_type":"text"},"source":["Optionally, use the Homography matrix inverse to draw lines on broadcast image"]},{"cell_type":"code","metadata":{"id":"1ZXLhK2OQoha","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}